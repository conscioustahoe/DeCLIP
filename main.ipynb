{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from transformers.models.clip.modeling_clip import CLIPEncoderLayer\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeCLIPDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, processor=None):\n",
    "        self.dataset = list(load_dataset(\"detection-datasets/coco\", split=\"validation\", streaming=True).take(num_samples))\n",
    "        self.processor = processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataset[idx]['image']\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        crops, coords = self.generate_grid_crops(image)\n",
    "        \n",
    "        processed_image = self.processor(images=image, return_tensors=\"pt\")\n",
    "        processed_crops = [self.processor(images=crop, return_tensors=\"pt\") for crop in crops]\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': processed_image['pixel_values'].squeeze(0),\n",
    "            'crop_pixel_values': torch.stack([c['pixel_values'].squeeze(0) for c in processed_crops]),\n",
    "            'crop_coords': torch.tensor(coords, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    def generate_grid_crops(self, image, grid_size=3):\n",
    "        w, h = image.size\n",
    "        crop_w, crop_h = w // grid_size, h // grid_size\n",
    "        \n",
    "        crops = []\n",
    "        coords = []\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                x1 = j * crop_w\n",
    "                y1 = i * crop_h\n",
    "                x2 = min(x1 + crop_w, w)\n",
    "                y2 = min(y1 + crop_h, h)\n",
    "                \n",
    "                crop = image.crop((x1, y1, x2, y2))\n",
    "                crops.append(crop)\n",
    "                coords.append([x1/w, y1/h, x2/w, y2/h])\n",
    "                \n",
    "        return crops, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoupledCLIPEncoderLayer(nn.Module):\n",
    "    def __init__(self, original_layer):\n",
    "        super().__init__()\n",
    "        self.self_attn = original_layer.self_attn\n",
    "        self.layer_norm1 = original_layer.layer_norm1\n",
    "        self.mlp = original_layer.mlp\n",
    "        self.layer_norm2 = original_layer.layer_norm2\n",
    "        \n",
    "        config = original_layer.self_attn.config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.q_proj.weight.copy_(self.self_attn.q_proj.weight)\n",
    "            self.q_proj.bias.copy_(self.self_attn.q_proj.bias)\n",
    "            self.v_proj.weight.copy_(self.self_attn.v_proj.weight)\n",
    "            self.v_proj.bias.copy_(self.self_attn.v_proj.bias)\n",
    "            self.out_proj.weight.copy_(self.self_attn.out_proj.weight)\n",
    "            self.out_proj.bias.copy_(self.self_attn.out_proj.bias)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None, causal_attention_mask=None, output_attentions=False):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        \n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        context_features = self.q_proj(hidden_states)\n",
    "        value_features = self.v_proj(hidden_states)\n",
    "        \n",
    "        context_features = context_features.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_features = value_features.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attention_weights = torch.matmul(context_features, context_features.transpose(-1, -2)) / np.sqrt(self.head_dim)\n",
    "        attention_probs = F.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "        content_features = torch.matmul(attention_probs, value_features)\n",
    "        content_features = content_features.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        content_features = self.out_proj(content_features)\n",
    "        \n",
    "        hidden_states = residual + content_features\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        outputs = (hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (attention_probs.mean(dim=1),)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeCLIPModel(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.original_last_layer = self.clip.vision_model.encoder.layers[-1]\n",
    "        self.clip.vision_model.encoder.layers[-1] = DecoupledCLIPEncoderLayer(self.original_last_layer)\n",
    "        \n",
    "        for param in self.clip.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.clip.vision_model.encoder.layers[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, pixel_values, output_attentions=False):\n",
    "        outputs = self.clip.vision_model(pixel_values=pixel_values, output_attentions=output_attentions)\n",
    "        return outputs\n",
    "    \n",
    "    def get_image_features(self, pixel_values):\n",
    "        outputs = self.forward(pixel_values)\n",
    "        return self.clip.visual_projection(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_pool(features, coords, output_size=1):\n",
    "    batch_size, num_crops = coords.shape[0], coords.shape[1]\n",
    "    pooled_features = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for c in range(num_crops):\n",
    "            x1, y1, x2, y2 = coords[b, c]\n",
    "            h_start = int(y1 * 14)\n",
    "            h_end = int(y2 * 14)\n",
    "            w_start = int(x1 * 14)\n",
    "            w_end = int(x2 * 14)\n",
    "            \n",
    "            region_features = features[b, :, h_start:h_end, w_start:w_end]\n",
    "            pooled = F.adaptive_avg_pool2d(region_features.unsqueeze(0), output_size)\n",
    "            pooled_features.append(pooled.squeeze(0))\n",
    "    \n",
    "    return torch.stack(pooled_features).view(batch_size, num_crops, -1)\n",
    "\n",
    "def compute_content_loss(student_model, teacher_model, images, crops, coords):\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = []\n",
    "        for b in range(crops.shape[0]):\n",
    "            crop_features = teacher_model.get_image_features(crops[b])\n",
    "            teacher_outputs.append(crop_features)\n",
    "        teacher_features = torch.stack(teacher_outputs)\n",
    "    \n",
    "    student_outputs = student_model(images)\n",
    "    student_hidden = student_outputs.last_hidden_state\n",
    "    \n",
    "    B, L, D = student_hidden.shape\n",
    "    H = W = int(np.sqrt(L - 1))\n",
    "    student_spatial = student_hidden[:, 1:, :].transpose(1, 2).view(B, D, H, W)\n",
    "    \n",
    "    student_spatial_normalized = F.normalize(student_model.clip.visual_projection(student_spatial.view(B, D, -1).transpose(1, 2)), dim=-1)\n",
    "    student_spatial_normalized = student_spatial_normalized.transpose(1, 2).view(B, -1, H, W)\n",
    "    \n",
    "    student_regions = roi_pool(student_spatial_normalized, coords)\n",
    "    \n",
    "    teacher_features_norm = F.normalize(teacher_features, dim=-1)\n",
    "    student_regions_norm = F.normalize(student_regions, dim=-1)\n",
    "    \n",
    "    similarity = F.cosine_similarity(student_regions_norm, teacher_features_norm, dim=-1)\n",
    "    loss = 1 - similarity.mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_context_loss(student_outputs, teacher_model, images):\n",
    "    student_hidden = student_outputs.last_hidden_state[:, 1:, :]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model(images, output_hidden_states=True)\n",
    "        teacher_hidden = teacher_outputs.hidden_states[-1][:, 1:, :]\n",
    "    \n",
    "    student_corr = F.cosine_similarity(student_hidden.unsqueeze(1), student_hidden.unsqueeze(2), dim=-1)\n",
    "    teacher_corr = F.cosine_similarity(teacher_hidden.unsqueeze(1), teacher_hidden.unsqueeze(2), dim=-1)\n",
    "    \n",
    "    context_loss = F.mse_loss(student_corr, teacher_corr)\n",
    "    \n",
    "    return context_loss\n",
    "\n",
    "def visualize_attention_maps(model, images, title=\"Attention Maps\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images, output_attentions=True)\n",
    "        attentions = outputs.attentions[-1]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(8, images.shape[0])):\n",
    "        attn = attentions[i].mean(dim=0)[50, 1:].reshape(14, 14)\n",
    "        axes[i].imshow(attn.cpu().numpy(), cmap='hot', interpolation='bilinear')\n",
    "        axes[i].set_title(f'Image {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_region_classification(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            crops = batch['crop_pixel_values'].to(device)\n",
    "            coords = batch['crop_coords'].to(device)\n",
    "            \n",
    "            for b in range(images.shape[0]):\n",
    "                student_outputs = model(images[b:b+1])\n",
    "                student_hidden = student_outputs.last_hidden_state\n",
    "                \n",
    "                B, L, D = student_hidden.shape\n",
    "                H = W = int(np.sqrt(L - 1))\n",
    "                student_spatial = student_hidden[:, 1:, :].transpose(1, 2).view(B, D, H, W)\n",
    "                \n",
    "                student_spatial_normalized = F.normalize(\n",
    "                    model.clip.visual_projection(student_spatial.view(B, D, -1).transpose(1, 2)), \n",
    "                    dim=-1\n",
    "                ).transpose(1, 2).view(B, -1, H, W)\n",
    "                \n",
    "                student_regions = roi_pool(student_spatial_normalized, coords[b:b+1])\n",
    "                \n",
    "                crop_features = model.clip.get_image_features(crops[b])\n",
    "                \n",
    "                similarities = F.cosine_similarity(\n",
    "                    student_regions.squeeze(0), \n",
    "                    F.normalize(crop_features, dim=-1), \n",
    "                    dim=-1\n",
    "                )\n",
    "                \n",
    "                predictions = similarities.argmax(dim=0)\n",
    "                correct += (predictions == torch.arange(9).to(device)).sum().item()\n",
    "                total += 9\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_declip():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    teacher_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    teacher_clip.eval()\n",
    "    \n",
    "    teacher_dino = timm.create_model('vit_base_patch16_224.dino', pretrained=True, num_classes=0).to(device)\n",
    "    teacher_dino.eval()\n",
    "    \n",
    "    declip_model = DeCLIPModel(clip_model).to(device)\n",
    "    \n",
    "    dataset = DeCLIPDataset(num_samples=1000, processor=processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, declip_model.parameters()),\n",
    "        lr=1e-5,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    test_batch = next(iter(dataloader))\n",
    "    test_images = test_batch['pixel_values'][:8].to(device)\n",
    "    \n",
    "    print(\"Vanilla CLIP attention maps:\")\n",
    "    vanilla_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    visualize_attention_maps(vanilla_clip.vision_model, test_images, \"Vanilla CLIP - Before Training\")\n",
    "    \n",
    "    vanilla_acc = evaluate_region_classification(vanilla_clip.vision_model, DataLoader(dataset, batch_size=8), device)\n",
    "    print(f\"Vanilla CLIP region classification accuracy: {vanilla_acc:.4f}\")\n",
    "    \n",
    "    num_epochs = 2\n",
    "    lambda_context = 0.25\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        declip_model.train()\n",
    "        total_loss = 0\n",
    "        content_losses = []\n",
    "        context_losses = []\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            crops = batch['crop_pixel_values'].to(device)\n",
    "            coords = batch['crop_coords'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            student_outputs = declip_model(images, output_attentions=True)\n",
    "            \n",
    "            content_loss = compute_content_loss(declip_model, teacher_clip, images, crops, coords)\n",
    "            context_loss = compute_context_loss(student_outputs, teacher_dino, images)\n",
    "            \n",
    "            loss = content_loss + lambda_context * context_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            content_losses.append(content_loss.item())\n",
    "            context_losses.append(context_loss.item())\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Content': f'{content_loss.item():.4f}',\n",
    "                'Context': f'{context_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_content = np.mean(content_losses)\n",
    "        avg_context = np.mean(context_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}, Content: {avg_content:.4f}, Context: {avg_context:.4f}\")\n",
    "    \n",
    "    print(\"DeCLIP attention maps after training:\")\n",
    "    visualize_attention_maps(declip_model, test_images, \"DeCLIP - After Training\")\n",
    "    \n",
    "    declip_acc = evaluate_region_classification(declip_model, DataLoader(dataset, batch_size=8), device)\n",
    "    print(f\"DeCLIP region classification accuracy: {declip_acc:.4f}\")\n",
    "    print(f\"Improvement: {(declip_acc - vanilla_acc) * 100:.2f}%\")\n",
    "    \n",
    "    return declip_model, vanilla_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "declip_model, vanilla_clip = train_declip()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
